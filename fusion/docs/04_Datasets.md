# Module 04: Datasets

## 1. Rationale & Purpose

The credibility of our MVP rests on the quality and relevance of the data used for testing. This directory serves as the central repository and documentation hub for all datasets used in the 48CB project. Its purpose is to ensure that our data is clean, well-documented, version-controlled, and accessible in a standardized format.

We will use two classes of data:
1.  **Historical Data:** To prove that our controller can handle the complexity and noise of real-world fusion experiments.
2.  **Synthetic Data:** To perform controlled, large-scale experiments and ablation studies that are not possible with limited historical data.

This "data-as-a-deliverable" approach is critical for transparency and reproducibility.

## 2. Data Specification

All datasets will be stored in the **Apache Parquet** format. This format is highly efficient for columnar data, supports schema evolution, and is the standard for serious data engineering. Each dataset will be accompanied by a small YAML file defining its schema, the physical meaning of each channel, and its provenance.

### 2.1. Historical Dataset (`historical_d3d_v1.0.parquet`)
- **Source:** DIII-D Tokamak (publicly available data via the MDSplus database). We will select a specific, versioned subset of shots.
- **Content:**
    - **Shot Selection:** A curated list of ~500 shot numbers, including:
        - ~50 documented disruptive shots.
        - ~50 shots documented as "near-misses" or having significant MHD activity.
        - ~400 stable, high-performance H-mode shots.
    - **Channels (Signals):** A minimal set of 5-10 core signals, sampled at 1kHz for the full duration of each shot. The exact list is TBD but will include:
        - `Ip`: Plasma Current
        - `n_e`: Line-averaged electron density
        - `W_mhd`: Stored plasma energy
        - `q95`: Safety factor at the 95% flux surface
        - `li`: Internal inductance
        - Locked mode detector signal
        - A representative Mirnov coil signal
    - **Metadata:** For each shot, we will include the official disruption time (if any) and a brief description of the shot's characteristics.

### 2.2. Synthetic Dataset (`synthetic_tokamak_v1.0.parquet`)
- **Source:** Generated by the `TokamakSim-lite` simulator from Module 02.
- **Content:**
    - **~6,000 simulated shots ("episodes").**
    - Each shot will have a duration of 2 seconds, sampled at 1kHz, with ~60 channels (including both physical states and diagnostic outputs).
    - The dataset will be balanced to include a variety of scenarios:
        - Shots that disrupt if uncontrolled.
        - Shots that are stable by default.
        - Shots that are sensitive to controller actions (can be saved or caused to disrupt).
    - The ground truth "temperature" (the true instability risk) will be included as a channel, allowing us to directly evaluate the performance of our estimators.

### 2.3. Synthetic ICF Dataset (`synthetic_icf_v1.0.parquet`)
- **Source:** Generated by the `ICFSim-lite` simulator from Module 02.
- **Content:**
    - **~15,000 simulated shots.**
    - Each shot will be a short time-series (~200 steps) with ~30 channels, representing the evolution of the implosion asymmetry.
    - The dataset will include a variety of initial seed asymmetries and drive conditions.
    - The ground truth asymmetry evolution will be included for direct comparison with the controller's performance.

## 3. Data Access & Management
- **Storage:** For development, data will be stored locally. For collaboration or larger scales, it will be stored in a versioned object store (like AWS S3 or Google Cloud Storage).
- **Versioning:** Any change to the data processing pipeline that results in a different dataset will trigger a new version number (e.g., `v1.1`) to ensure that all experiments are linked to a specific, immutable version of the data.

## 4. Success Criteria
- The data pipeline for acquiring and cleaning the historical DIII-D data is fully automated and reproducible.
- The schemas for all datasets are explicitly defined and version-controlled.
- The synthetic datasets are large enough to produce statistically significant results in our benchmark studies.
- The datasets are packaged in a way that makes it easy for a third party to download and use them to replicate our findings.
